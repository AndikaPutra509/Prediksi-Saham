{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtXqJwqGBoDHxeVSTrUc+G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndikaPutra509/Prediksi-Saham/blob/PrediksiSaham/PREDIKSI_SAHAM_GACOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "v7BrpZbiJAft"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.check_call(\"pip install ta\", shell=True)\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ta\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    f1_score,\n",
        "    balanced_accuracy_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "from sklearn.ensemble import ExtraTreesClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "ekSbLk3DJHe3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pilih mode: \"harian\" (intraday/hourly) atau \"mingguan\" (daily swing)\n",
        "TRADING_MODE = \"mingguan\""
      ],
      "metadata": {
        "id": "eB0QqKPfJMWh"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYMBOL = \"HUMI.JK\"\n",
        "INTERVAL = \"1h\"\n",
        "START = \"2013-01-01\"\n",
        "END = \"2026-02-28\"\n",
        "INTRADAY_PERIOD = \"365d\"\n",
        "INTRADAY_FALLBACK_PERIODS = [\"365d\", \"180d\", \"90d\", \"60d\", \"30d\", \"10d\", \"5d\", \"3d\"]\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.15\n",
        "HOLD_BAND = 0.05\n",
        "TARGET_MODE = \"three_class\"  # \"binary\" atau \"three_class\"\n",
        "RETURN_THRESHOLD = 0.01\n",
        "USE_PROB_CALIBRATION = True\n",
        "MIN_ROWS_AFTER_FEATURES = 40\n",
        "MIN_ROWS_INTRADAY = 18\n",
        "MIN_RAW_ROWS_FOR_FEATURES = 20\n",
        "AS_OF_DATE = None  # contoh: \"2026-02-18\" atau \"2026-02-18 15:00:00\"\n",
        "IHSG_TICKER = \"^JKSE\"\n",
        "INDO_VIX_CANDIDATES = [\"^JKVIX\", \"JKVIX\", \"VIX.JK\", \"^VIX\", \"VIX\"]\n",
        "GLOBAL_INDEX_TICKERS = {\n",
        "    \"SP500\": \"^GSPC\",\n",
        "    \"NASDAQ\": \"^IXIC\",\n",
        "    \"DOWJONES\": \"^DJI\",\n",
        "    \"NIKKEI\": \"^N225\",\n",
        "    \"SHANGHAI\": \"000001.SS\",\n",
        "}\n",
        "COMMODITY_TICKERS = {\n",
        "    \"OIL\": \"CL=F\",\n",
        "    \"GOLD\": \"GC=F\",\n",
        "}\n",
        "VIX_SPIKE_THRESHOLD = 0.04  # lonjakan 1-periode (4%) dianggap fear naik\n",
        "VIX_HIGH_LEVEL = 25.0\n",
        "USDIDR_TICKER = \"IDR=X\"\n",
        "JKT_TZ = \"Asia/Jakarta\"\n",
        "SESSION1_START = \"09:00:00\"\n",
        "SESSION1_END = \"12:00:00\"\n",
        "SESSION2_START = \"13:30:00\"\n",
        "SESSION2_END = \"16:00:00\"\n",
        "SR_LOOKBACK_DAILY = 20\n",
        "SR_LOOKBACK_INTRADAY = 8"
      ],
      "metadata": {
        "id": "qWsUErMJJNHw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve_trading_mode(mode: str) -> dict:\n",
        "    normalized = str(mode).strip().lower()\n",
        "    if normalized not in {\"harian\", \"mingguan\"}:\n",
        "        raise ValueError('TRADING_MODE harus \"harian\" atau \"mingguan\".')\n",
        "\n",
        "    if normalized == \"harian\":\n",
        "        return {\n",
        "            \"name\": \"harian\",\n",
        "            \"interval\": \"1h\",\n",
        "            \"forecast_periods\": 24,\n",
        "            \"forecast_label\": \"Prediksi harga 24 jam ke depan (hourly)\",\n",
        "            \"horizon_note\": \"intraday/day-trading\",\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"name\": \"mingguan\",\n",
        "        \"interval\": \"1d\",\n",
        "        \"forecast_periods\": 5,\n",
        "        \"forecast_label\": \"Prediksi harga 1 minggu ke depan (5 hari bursa)\",\n",
        "        \"horizon_note\": \"weekly swing-trading\",\n",
        "    }\n",
        "\n",
        "\n",
        "def get_runtime_mode_config() -> dict:\n",
        "    \"\"\"Ambil config mode terbaru setiap eksekusi (aman untuk notebook/interaktif).\"\"\"\n",
        "    return resolve_trading_mode(TRADING_MODE)\n",
        "\n",
        "\n",
        "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = df.columns.get_level_values(0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def replace_inf_with_nan(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    num_cols = out.select_dtypes(include=[np.number]).columns\n",
        "    out.loc[:, num_cols] = out.loc[:, num_cols].replace([np.inf, -np.inf], np.nan)\n",
        "    return out\n",
        "\n",
        "\n",
        "def download_data(symbol: str, interval: str, start: str, end: str, intraday_period: str) -> pd.DataFrame:\n",
        "    if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}:\n",
        "        raw = yf.download(symbol, period=intraday_period, interval=interval, auto_adjust=True, progress=True)\n",
        "    else:\n",
        "        raw = yf.download(symbol, start=start, end=end, interval=interval, auto_adjust=True, progress=True)\n",
        "    raw = normalize_columns(raw)\n",
        "    raw = filter_jakarta_sessions(raw, interval)\n",
        "    return raw\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def apply_as_of_cutoff(df: pd.DataFrame, as_of_date: str | None) -> pd.DataFrame:\n",
        "    if as_of_date is None:\n",
        "        return df\n",
        "\n",
        "    cutoff = pd.to_datetime(as_of_date)\n",
        "\n",
        "    # Jika user isi tanggal tanpa jam, anggap sampai akhir hari itu\n",
        "    if len(str(as_of_date)) <= 10:\n",
        "        cutoff = cutoff + pd.Timedelta(days=1) - pd.Timedelta(microseconds=1)\n",
        "\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex) and idx.tz is not None and cutoff.tzinfo is None:\n",
        "        cutoff = cutoff.tz_localize(idx.tz)\n",
        "\n",
        "    return df.loc[df.index <= cutoff].copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def apply_idx_to_jakarta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if not isinstance(out.index, pd.DatetimeIndex):\n",
        "        return out\n",
        "    if out.index.tz is None:\n",
        "        out.index = out.index.tz_localize(JKT_TZ)\n",
        "    else:\n",
        "        out.index = out.index.tz_convert(JKT_TZ)\n",
        "    return out\n",
        "\n",
        "\n",
        "def filter_jakarta_sessions(df: pd.DataFrame, interval: str) -> pd.DataFrame:\n",
        "    \"\"\"Filter data agar hanya jam perdagangan BEI sesi 1 & 2.\"\"\"\n",
        "    if interval not in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}:\n",
        "        return df\n",
        "\n",
        "    out = apply_idx_to_jakarta(df)\n",
        "    sess1 = out.between_time(SESSION1_START, SESSION1_END, inclusive=\"both\")\n",
        "    sess2 = out.between_time(SESSION2_START, SESSION2_END, inclusive=\"both\")\n",
        "    out = pd.concat([sess1, sess2]).sort_index()\n",
        "\n",
        "    # buang akhir pekan bila ada\n",
        "    if isinstance(out.index, pd.DatetimeIndex):\n",
        "        out = out[out.index.dayofweek < 5]\n",
        "    return out\n",
        "\n",
        "\n",
        "def try_download_close_series(symbol: str, interval: str, start: str, end: str, intraday_period: str, as_of_date: str | None) -> pd.Series | None:\n",
        "    try:\n",
        "        raw = download_data(symbol, interval, start, end, intraday_period)\n",
        "        raw = apply_as_of_cutoff(raw, as_of_date)\n",
        "        if raw.empty or \"Close\" not in raw.columns:\n",
        "            return None\n",
        "        ser = raw[\"Close\"].copy()\n",
        "        ser.name = symbol\n",
        "        return ser\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def add_market_context_features(df: pd.DataFrame, interval: str, start: str, end: str, intraday_period: str, as_of_date: str | None):\n",
        "    out = df.copy()\n",
        "    info = {\n",
        "        \"ihsg_used\": False,\n",
        "        \"indovix_symbol\": None,\n",
        "        \"currency_used\": False,\n",
        "        \"global_markets_used\": 0,\n",
        "        \"commodities_used\": 0,\n",
        "        \"timestamps_aligned\": True,\n",
        "    }\n",
        "\n",
        "    is_intraday = interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}\n",
        "    ctx_roll = 12 if is_intraday else 20\n",
        "\n",
        "    ihsg_close = try_download_close_series(IHSG_TICKER, interval, start, end, intraday_period, as_of_date)\n",
        "    if ihsg_close is not None:\n",
        "        ihsg_close = ihsg_close.reindex(out.index).ffill().bfill()\n",
        "        ihsg_ret = ihsg_close.pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "        ihsg_ma20 = ihsg_close.rolling(ctx_roll).mean().replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        out[\"IHSG_Return_1\"] = ihsg_ret\n",
        "        out[\"IHSG_Return_5\"] = ihsg_close.pct_change(5).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "        out[\"IHSG_Volatility_10\"] = ihsg_ret.rolling(10).std().fillna(0.0)\n",
        "        out[\"IHSG_MA20\"] = ihsg_ma20.ffill().bfill()\n",
        "        out[\"IHSG_Trend\"] = (ihsg_close / ihsg_ma20).replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
        "        out[\"Market_Risk_Regime\"] = (out[\"IHSG_Trend\"] >= 1.0).astype(int)\n",
        "        info[\"ihsg_used\"] = True\n",
        "    else:\n",
        "        out[\"IHSG_Return_1\"] = 0.0\n",
        "        out[\"IHSG_Return_5\"] = 0.0\n",
        "        out[\"IHSG_Volatility_10\"] = 0.0\n",
        "        out[\"IHSG_MA20\"] = 0.0\n",
        "        out[\"IHSG_Trend\"] = 1.0\n",
        "        out[\"Market_Risk_Regime\"] = 0\n",
        "\n",
        "    vix_close = None\n",
        "    for ticker in INDO_VIX_CANDIDATES:\n",
        "        candidate = try_download_close_series(ticker, interval, start, end, intraday_period, as_of_date)\n",
        "        if candidate is not None:\n",
        "            vix_close = candidate\n",
        "            info[\"indovix_symbol\"] = ticker\n",
        "            break\n",
        "\n",
        "    if vix_close is not None:\n",
        "        vix_close = vix_close.reindex(out.index).ffill().bfill()\n",
        "        vix_change = vix_close.pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "        out[\"INDOVIX_Level\"] = vix_close.ffill().bfill()\n",
        "        out[\"INDOVIX_Change_1\"] = vix_change\n",
        "        out[\"INDOVIX_Change_5\"] = vix_close.pct_change(5).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "        out[\"VIX_Change\"] = out[\"INDOVIX_Change_1\"]\n",
        "        out[\"VIX_Spike\"] = (out[\"VIX_Change\"] > VIX_SPIKE_THRESHOLD).astype(int)\n",
        "        out[\"VIX_Level\"] = out[\"INDOVIX_Level\"]\n",
        "    else:\n",
        "        out[\"INDOVIX_Level\"] = 0.0\n",
        "        out[\"INDOVIX_Change_1\"] = 0.0\n",
        "        out[\"INDOVIX_Change_5\"] = 0.0\n",
        "        out[\"VIX_Change\"] = 0.0\n",
        "        out[\"VIX_Spike\"] = 0\n",
        "        out[\"VIX_Level\"] = 0.0\n",
        "\n",
        "    # Global market + commodity context\n",
        "    market_return_cols = []\n",
        "    for market_name, market_ticker in GLOBAL_INDEX_TICKERS.items():\n",
        "        market_close = try_download_close_series(market_ticker, interval, start, end, intraday_period, as_of_date)\n",
        "        prefix = f\"GM_{market_name}\"\n",
        "        if market_close is not None:\n",
        "            market_close = market_close.reindex(out.index).ffill().bfill()\n",
        "            market_ret_1 = market_close.pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "            market_ret_5 = market_close.pct_change(5).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "            market_trend = (market_close / market_close.rolling(ctx_roll).mean()).replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
        "            out[f\"{prefix}_Return_1\"] = market_ret_1\n",
        "            out[f\"{prefix}_Return_5\"] = market_ret_5\n",
        "            out[f\"{prefix}_Trend\"] = market_trend\n",
        "            market_return_cols.append(f\"{prefix}_Return_1\")\n",
        "            info[\"global_markets_used\"] += 1\n",
        "        else:\n",
        "            out[f\"{prefix}_Return_1\"] = 0.0\n",
        "            out[f\"{prefix}_Return_5\"] = 0.0\n",
        "            out[f\"{prefix}_Trend\"] = 1.0\n",
        "\n",
        "    commodity_return_cols = []\n",
        "    for commodity_name, commodity_ticker in COMMODITY_TICKERS.items():\n",
        "        commodity_close = try_download_close_series(commodity_ticker, interval, start, end, intraday_period, as_of_date)\n",
        "        prefix = f\"CMDTY_{commodity_name}\"\n",
        "        if commodity_close is not None:\n",
        "            commodity_close = commodity_close.reindex(out.index).ffill().bfill()\n",
        "            commodity_ret_1 = commodity_close.pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "            commodity_ret_5 = commodity_close.pct_change(5).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "            commodity_trend = (commodity_close / commodity_close.rolling(ctx_roll).mean()).replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
        "            out[f\"{prefix}_Return_1\"] = commodity_ret_1\n",
        "            out[f\"{prefix}_Return_5\"] = commodity_ret_5\n",
        "            out[f\"{prefix}_Trend\"] = commodity_trend\n",
        "            commodity_return_cols.append(f\"{prefix}_Return_1\")\n",
        "            info[\"commodities_used\"] += 1\n",
        "        else:\n",
        "            out[f\"{prefix}_Return_1\"] = 0.0\n",
        "            out[f\"{prefix}_Return_5\"] = 0.0\n",
        "            out[f\"{prefix}_Trend\"] = 1.0\n",
        "\n",
        "    if market_return_cols:\n",
        "        out[\"Global_Market_Return_Composite\"] = out[market_return_cols].mean(axis=1)\n",
        "        out[\"Global_Market_Stress\"] = out[market_return_cols].std(axis=1).fillna(0.0)\n",
        "    else:\n",
        "        out[\"Global_Market_Return_Composite\"] = 0.0\n",
        "        out[\"Global_Market_Stress\"] = 0.0\n",
        "\n",
        "    if commodity_return_cols:\n",
        "        out[\"Commodity_Return_Composite\"] = out[commodity_return_cols].mean(axis=1)\n",
        "    else:\n",
        "        out[\"Commodity_Return_Composite\"] = 0.0\n",
        "\n",
        "    # Interaction: kombinasikan global risk proxy dengan INDO VIX\n",
        "    vix_proxy = out[\"INDOVIX_Change_1\"].clip(-0.3, 0.3)\n",
        "    out[\"Global_VIX_Interaction\"] = out[\"Global_Market_Return_Composite\"] * vix_proxy\n",
        "    out[\"Commodity_VIX_Interaction\"] = out[\"Commodity_Return_Composite\"] * vix_proxy\n",
        "\n",
        "    # Currency context (USD/IDR)\n",
        "    usdidr_close = try_download_close_series(USDIDR_TICKER, interval, start, end, intraday_period, as_of_date)\n",
        "    if usdidr_close is not None:\n",
        "        usdidr_close = usdidr_close.reindex(out.index).ffill().bfill()\n",
        "        usdidr_ret = usdidr_close.pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "        out[\"USDIDR_Level\"] = usdidr_close\n",
        "        out[\"USDIDR_Return_1\"] = usdidr_ret\n",
        "        out[\"USDIDR_Return_5\"] = usdidr_close.pct_change(5).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "        out[\"USDIDR_Trend\"] = (usdidr_close / usdidr_close.rolling(ctx_roll).mean()).replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
        "        info[\"currency_used\"] = True\n",
        "    else:\n",
        "        out[\"USDIDR_Level\"] = 0.0\n",
        "        out[\"USDIDR_Return_1\"] = 0.0\n",
        "        out[\"USDIDR_Return_5\"] = 0.0\n",
        "        out[\"USDIDR_Trend\"] = 1.0\n",
        "\n",
        "    # Hindari drop semua baris saat beberapa sumber eksternal kosong/tidak sinkron.\n",
        "    out = replace_inf_with_nan(out)\n",
        "    out = out.ffill().bfill()\n",
        "\n",
        "    for col in out.columns:\n",
        "        if out[col].isna().all():\n",
        "            out[col] = 1.0 if (\"Trend\" in col or col.endswith(\"_Level\")) else 0.0\n",
        "\n",
        "    num_cols = out.select_dtypes(include=[np.number]).columns\n",
        "    out.loc[:, num_cols] = out.loc[:, num_cols].fillna(0.0)\n",
        "\n",
        "    # Pastikan target tetap valid; jika NaN lebih baik dibuang spesifik di target saja.\n",
        "    if \"Target\" in out.columns:\n",
        "        out = out[out[\"Target\"].notna()].copy()\n",
        "\n",
        "    return out, info\n",
        "\n",
        "\n",
        "def build_features(df: pd.DataFrame, interval: str = \"1d\") -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if len(out) < MIN_RAW_ROWS_FOR_FEATURES:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    close = out[\"Close\"]\n",
        "    high = out[\"High\"]\n",
        "    low = out[\"Low\"]\n",
        "    volume = out[\"Volume\"]\n",
        "\n",
        "    if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}:\n",
        "        ma_short, ma_long = 8, 20\n",
        "        ret_w1, ret_w2 = 3, 6\n",
        "        vol_w1, vol_w2 = 6, 12\n",
        "        lag_list = [1, 2, 3, 4, 6]\n",
        "        roll_ctx = 12\n",
        "    else:\n",
        "        ma_short, ma_long = 20, 50\n",
        "        ret_w1, ret_w2 = 5, 10\n",
        "        vol_w1, vol_w2 = 10, 20\n",
        "        lag_list = [1, 2, 3, 5, 10]\n",
        "        roll_ctx = 20\n",
        "\n",
        "    out[\"RSI\"] = ta.momentum.RSIIndicator(close).rsi()\n",
        "    out[\"MA20\"] = close.rolling(ma_short).mean()\n",
        "    out[\"MA50\"] = close.rolling(ma_long).mean()\n",
        "    out[\"Trend_Filter\"] = (out[\"MA20\"] > out[\"MA50\"]).astype(int)\n",
        "    out[\"MACD\"] = ta.trend.MACD(close).macd()\n",
        "\n",
        "    bb = ta.volatility.BollingerBands(close)\n",
        "    out[\"BB_high\"] = bb.bollinger_hband()\n",
        "    out[\"BB_low\"] = bb.bollinger_lband()\n",
        "    out[\"BB_width\"] = out[\"BB_high\"] - out[\"BB_low\"]\n",
        "\n",
        "    atr_window = max(2, min(14, len(out)))\n",
        "    out[\"ATR\"] = ta.volatility.AverageTrueRange(high, low, close, window=atr_window).average_true_range()\n",
        "    out[\"OBV\"] = ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()\n",
        "\n",
        "    out[\"Return\"] = close.pct_change()\n",
        "    out[\"Return_5\"] = close.pct_change(ret_w1)\n",
        "    out[\"Return_10\"] = close.pct_change(ret_w2)\n",
        "    out[\"Volatility_10\"] = out[\"Return\"].rolling(vol_w1).std()\n",
        "    out[\"Volatility_20\"] = out[\"Return\"].rolling(vol_w2).std()\n",
        "    out[\"Volume_Change\"] = volume.pct_change()\n",
        "\n",
        "    for lag in lag_list:\n",
        "        out[f\"Lag_Return_{lag}\"] = out[\"Return\"].shift(lag)\n",
        "        out[f\"Lag_RSI_{lag}\"] = out[\"RSI\"].shift(lag)\n",
        "\n",
        "\n",
        "    # Volume anomaly + buyer/seller pressure proxy\n",
        "    out[\"Candle_Direction\"] = np.sign(out[\"Close\"] - out[\"Open\"])\n",
        "    out[\"Buy_Volume_Proxy\"] = np.where(out[\"Candle_Direction\"] > 0, out[\"Volume\"], 0.0)\n",
        "    out[\"Sell_Volume_Proxy\"] = np.where(out[\"Candle_Direction\"] < 0, out[\"Volume\"], 0.0)\n",
        "\n",
        "    vol_mean_20 = out[\"Volume\"].rolling(roll_ctx).mean()\n",
        "    vol_std_20 = out[\"Volume\"].rolling(roll_ctx).std().replace(0, np.nan)\n",
        "    out[\"Volume_ZScore\"] = ((out[\"Volume\"] - vol_mean_20) / vol_std_20).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    buy_mean_20 = out[\"Buy_Volume_Proxy\"].rolling(roll_ctx).mean()\n",
        "    buy_std_20 = out[\"Buy_Volume_Proxy\"].rolling(roll_ctx).std().replace(0, np.nan)\n",
        "    sell_mean_20 = out[\"Sell_Volume_Proxy\"].rolling(roll_ctx).mean()\n",
        "    sell_std_20 = out[\"Sell_Volume_Proxy\"].rolling(roll_ctx).std().replace(0, np.nan)\n",
        "\n",
        "    out[\"Buy_Volume_Anomaly\"] = ((out[\"Buy_Volume_Proxy\"] - buy_mean_20) / buy_std_20).replace([np.inf, -np.inf], np.nan)\n",
        "    out[\"Sell_Volume_Anomaly\"] = ((out[\"Sell_Volume_Proxy\"] - sell_mean_20) / sell_std_20).replace([np.inf, -np.inf], np.nan)\n",
        "    out[\"Net_Volume_Anomaly\"] = out[\"Buy_Volume_Anomaly\"] - out[\"Sell_Volume_Anomaly\"]\n",
        "    out[\"Volume_Anomaly_Spike\"] = (out[\"Volume_ZScore\"] > 2.0).astype(int)\n",
        "\n",
        "    # Support / Resistance features (lebih realistis untuk intraday/daily)\n",
        "    if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}:\n",
        "        sr_base = SR_LOOKBACK_INTRADAY\n",
        "    else:\n",
        "        sr_base = SR_LOOKBACK_DAILY\n",
        "    sr_lookback = max(5, min(sr_base, max(5, len(out) - 1)))\n",
        "    out[\"Support_Level\"] = out[\"Low\"].rolling(sr_lookback).min()\n",
        "    out[\"Resistance_Level\"] = out[\"High\"].rolling(sr_lookback).max()\n",
        "    out[\"Distance_To_Support\"] = (out[\"Close\"] - out[\"Support_Level\"]) / out[\"Close\"]\n",
        "    out[\"Distance_To_Resistance\"] = (out[\"Resistance_Level\"] - out[\"Close\"]) / out[\"Close\"]\n",
        "    out[\"Support_Break\"] = (out[\"Close\"] < out[\"Support_Level\"] * 0.999).astype(int)\n",
        "    out[\"Resistance_Break\"] = (out[\"Close\"] > out[\"Resistance_Level\"] * 1.001).astype(int)\n",
        "\n",
        "    out[\"Future_Return\"] = out[\"Return\"].shift(-1)\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        conditions = [\n",
        "            out[\"Future_Return\"] < -RETURN_THRESHOLD,\n",
        "            out[\"Future_Return\"].between(-RETURN_THRESHOLD, RETURN_THRESHOLD, inclusive=\"both\"),\n",
        "            out[\"Future_Return\"] > RETURN_THRESHOLD,\n",
        "        ]\n",
        "        out[\"Target\"] = np.select(conditions, [0, 1, 2], default=1).astype(int)\n",
        "    else:\n",
        "        out[\"Target\"] = (out[\"Future_Return\"] > RETURN_THRESHOLD).astype(int)\n",
        "    out = replace_inf_with_nan(out)\n",
        "    return out.dropna().copy()\n",
        "\n",
        "\n",
        "def safe_auc(y_true: np.ndarray, probs: np.ndarray) -> float:\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        return float(\"nan\")\n",
        "    return roc_auc_score(y_true, probs)\n",
        "\n",
        "\n",
        "def split_time_series(df: pd.DataFrame):\n",
        "    n = len(df)\n",
        "    if n < 12:\n",
        "        raise ValueError(f\"Data terlalu sedikit setelah feature engineering: {n} baris. Coba perbesar rentang START/END, ganti TRADING_MODE, atau gunakan saham dengan histori lebih panjang.\")\n",
        "\n",
        "    train_end = int(n * TRAIN_RATIO)\n",
        "    val_end = int(n * (TRAIN_RATIO + VAL_RATIO))\n",
        "\n",
        "    # pastikan masing-masing split minimal 1\n",
        "    train_end = max(1, min(train_end, n - 2))\n",
        "    val_end = max(train_end + 1, min(val_end, n - 1))\n",
        "\n",
        "    train_df = df.iloc[:train_end]\n",
        "    val_df = df.iloc[train_end:val_end]\n",
        "    test_df = df.iloc[val_end:]\n",
        "\n",
        "    if len(train_df) == 0 or len(val_df) == 0 or len(test_df) == 0:\n",
        "        raise ValueError(\n",
        "            f\"Split menghasilkan data kosong (train={len(train_df)}, val={len(val_df)}, test={len(test_df)}).\"\n",
        "        )\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def find_best_threshold(y_true: np.ndarray, probs: np.ndarray) -> float:\n",
        "    unique_classes = np.unique(y_true)\n",
        "    if len(y_true) == 0 or len(unique_classes) < 2:\n",
        "        return 0.5\n",
        "\n",
        "    # Threshold search hanya relevan untuk kasus biner (0/1).\n",
        "    # Untuk multiclass (mis. three_class), pakai decision default.\n",
        "    if len(unique_classes) > 2:\n",
        "        return 0.5\n",
        "\n",
        "    thresholds = np.arange(0.30, 0.71, 0.01)\n",
        "    best_t, best_bacc, best_f1 = 0.5, -1.0, -1.0\n",
        "    for t in thresholds:\n",
        "        preds = (probs >= t).astype(int)\n",
        "        bacc = balanced_accuracy_score(y_true, preds)\n",
        "        f1 = f1_score(y_true, preds, zero_division=0)\n",
        "        if (bacc > best_bacc) or (np.isclose(bacc, best_bacc) and f1 > best_f1):\n",
        "            best_t, best_bacc, best_f1 = float(t), float(bacc), float(f1)\n",
        "    return best_t\n",
        "\n",
        "\n",
        "def decide_signal(prob_up: float, threshold: float, hold_band: float = HOLD_BAND) -> str:\n",
        "    upper = threshold + hold_band\n",
        "    lower = threshold - hold_band\n",
        "    if prob_up >= upper and prob_up > 0.5:\n",
        "        return \"BELI\"\n",
        "    if prob_up <= lower and prob_up < 0.5:\n",
        "        return \"JUAL\"\n",
        "    return \"TAHAN\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def interpret_volume_flow(net_anomaly: float, buy_anomaly: float, sell_anomaly: float) -> str:\n",
        "    if any(np.isnan(x) for x in [net_anomaly, buy_anomaly, sell_anomaly]):\n",
        "        return \"Volume flow tidak tersedia\"\n",
        "    if net_anomaly >= 1.0 and buy_anomaly > sell_anomaly:\n",
        "        return \"BUY PRESSURE DOMINAN (anomali beli kuat)\"\n",
        "    if net_anomaly <= -1.0 and sell_anomaly > buy_anomaly:\n",
        "        return \"SELL PRESSURE DOMINAN (anomali jual kuat)\"\n",
        "    return \"Volume flow netral/campuran\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def interpret_sr_breakout(close_price: float, support: float, resistance: float, tol: float = 0.001):\n",
        "    if any(np.isnan(x) for x in [close_price, support, resistance]):\n",
        "        return \"SR tidak tersedia\", 0\n",
        "\n",
        "    if close_price < support * (1 - tol):\n",
        "        return \"SUPPORT JEBOL (bearish breakout)\", -1\n",
        "    if close_price > resistance * (1 + tol):\n",
        "        return \"RESISTANCE JEBOL (bullish breakout)\", 1\n",
        "    return \"Masih di dalam range support-resistance\", 0\n",
        "\n",
        "\n",
        "def adjust_signal_with_vix_fear(signal: str, prob_up: float, vix_level: float | None, vix_change_1: float | None):\n",
        "    \"\"\"\n",
        "    Menyesuaikan signal dengan konteks fear dari Indonesia Volatility Index.\n",
        "    - Jika VIX melonjak/tinggi, sinyal BELI dibuat lebih defensif.\n",
        "    \"\"\"\n",
        "    if signal == \"BELI\":\n",
        "        spike = (vix_change_1 is not None) and (not np.isnan(vix_change_1)) and (vix_change_1 >= VIX_SPIKE_THRESHOLD)\n",
        "        high_level = (vix_level is not None) and (not np.isnan(vix_level)) and (vix_level >= VIX_HIGH_LEVEL)\n",
        "\n",
        "        if spike and high_level:\n",
        "            return \"JUAL\", \"Fear tinggi (VIX spike + level tinggi): BELI diturunkan jadi JUAL defensif\"\n",
        "        if spike or high_level:\n",
        "            return \"TAHAN\", \"Fear meningkat dari INDO VIX: BELI diturunkan jadi TAHAN\"\n",
        "\n",
        "    if signal == \"TAHAN\":\n",
        "        low_fear = (vix_level is not None) and (not np.isnan(vix_level)) and (vix_level < (VIX_HIGH_LEVEL * 0.8))\n",
        "        fear_drop = (vix_change_1 is not None) and (not np.isnan(vix_change_1)) and (vix_change_1 <= -0.03)\n",
        "        if low_fear and fear_drop and prob_up >= 0.58:\n",
        "            return \"BELI\", \"Fear menurun signifikan + probabilitas naik kuat: TAHAN dinaikkan jadi BELI\"\n",
        "\n",
        "    return signal, \"Tidak ada penyesuaian signal dari INDO VIX\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AdaptiveQuantEnsemble:\n",
        "    \"\"\"Ensemble quant adaptif: pembobotan model berbasis performa walk-forward + regularisasi probabilitas.\"\"\"\n",
        "\n",
        "    def __init__(self, models: list[tuple[str, object]], temperature: float = 0.85):\n",
        "        self.models = models\n",
        "        self.temperature = temperature\n",
        "        self.model_weights_: dict[str, float] = {}\n",
        "        self.fitted_models_: list[tuple[str, object]] = []\n",
        "\n",
        "    def _time_split(self, X, y):\n",
        "        n = len(y)\n",
        "        split_idx = max(20, int(n * 0.8))\n",
        "        split_idx = min(split_idx, n - 5) if n > 25 else max(1, n - 1)\n",
        "        return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_arr = np.asarray(X)\n",
        "        y_arr = np.asarray(y)\n",
        "\n",
        "        # kalibrasi bobot antar model dengan walk-forward holdout sederhana\n",
        "        X_sub, y_sub, X_hold, y_hold = self._time_split(X_arr, y_arr)\n",
        "        raw_scores = []\n",
        "        temp_models = []\n",
        "        for name, model in self.models:\n",
        "            model.fit(X_sub, y_sub)\n",
        "            p_hold = np.nan_to_num(model.predict_proba(X_hold)[:, 1], nan=0.5, posinf=1.0, neginf=0.0)\n",
        "            th = find_best_threshold(y_hold, p_hold)\n",
        "            pred_hold = (p_hold >= th).astype(int)\n",
        "            score = balanced_accuracy_score(y_hold, pred_hold) if len(np.unique(y_hold)) > 1 else 0.5\n",
        "            raw_scores.append(max(0.01, float(score)))\n",
        "            temp_models.append((name, model))\n",
        "\n",
        "        # softmax-like weighting agar stabil\n",
        "        score_arr = np.array(raw_scores, dtype=float)\n",
        "        score_arr = np.exp((score_arr - np.max(score_arr)) * 8.0)\n",
        "        score_arr = score_arr / np.sum(score_arr)\n",
        "        self.model_weights_ = {name: float(w) for (name, _), w in zip(temp_models, score_arr)}\n",
        "\n",
        "        # fit ulang seluruh model dengan full training\n",
        "        self.fitted_models_ = []\n",
        "        for name, model in self.models:\n",
        "            model.fit(X_arr, y_arr)\n",
        "            self.fitted_models_.append((name, model))\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not self.fitted_models_:\n",
        "            raise ValueError(\"AdaptiveQuantEnsemble belum di-fit.\")\n",
        "\n",
        "        probs = []\n",
        "        weights = []\n",
        "        for name, model in self.fitted_models_:\n",
        "            p = np.nan_to_num(model.predict_proba(X), nan=0.5, posinf=1.0, neginf=0.0)\n",
        "            probs.append(p)\n",
        "            weights.append(float(self.model_weights_.get(name, 1.0 / max(1, len(self.fitted_models_)))))\n",
        "\n",
        "        w = np.array(weights, dtype=float)\n",
        "        w = w / np.sum(w)\n",
        "\n",
        "        blended = np.zeros_like(probs[0], dtype=float)\n",
        "        for wi, pi in zip(w, probs):\n",
        "            blended += wi * pi\n",
        "\n",
        "        # realism: shrink probabilitas saat antar-model tidak sepakat (kurangi ekstrem bullish/bearish)\n",
        "        p_up_each = np.column_stack([p[:, 1] for p in probs])\n",
        "        disagreement = np.std(p_up_each, axis=1)\n",
        "        shrink = np.clip(disagreement * self.temperature, 0.0, 0.35)\n",
        "        p_up = blended[:, 1]\n",
        "        p_up = (1 - shrink) * p_up + shrink * 0.5\n",
        "        p_down = 1 - p_up\n",
        "        return np.column_stack([p_down, p_up])\n",
        "\n",
        "\n",
        "def build_adaptive_quant_ensemble(include_xgb: bool):\n",
        "    base_models = [\n",
        "        (\n",
        "            \"hgb\",\n",
        "            HistGradientBoostingClassifier(\n",
        "                learning_rate=0.03,\n",
        "                max_depth=4,\n",
        "                max_iter=450,\n",
        "                min_samples_leaf=20,\n",
        "                random_state=SEED,\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"et\",\n",
        "            ExtraTreesClassifier(\n",
        "                n_estimators=800,\n",
        "                max_depth=10,\n",
        "                min_samples_leaf=6,\n",
        "                class_weight=\"balanced_subsample\",\n",
        "                random_state=SEED,\n",
        "                n_jobs=-1,\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"lr\",\n",
        "            Pipeline(\n",
        "                steps=[\n",
        "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                    (\"scaler\", StandardScaler()),\n",
        "                    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=SEED)),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    if include_xgb and TARGET_MODE != \"three_class\":\n",
        "        try:\n",
        "            from xgboost import XGBClassifier\n",
        "\n",
        "            base_models.append(\n",
        "                (\n",
        "                    \"xgb\",\n",
        "                    XGBClassifier(\n",
        "                        n_estimators=500,\n",
        "                        learning_rate=0.03,\n",
        "                        max_depth=4,\n",
        "                        subsample=0.9,\n",
        "                        colsample_bytree=0.9,\n",
        "                        reg_lambda=1.0,\n",
        "                        objective=\"binary:logistic\",\n",
        "                        eval_metric=\"logloss\",\n",
        "                        random_state=SEED,\n",
        "                        n_jobs=-1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return AdaptiveQuantEnsemble(models=base_models, temperature=0.85)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RegimeAwareAdaptiveQuant:\n",
        "    \"\"\"Model quant regime-aware: gabungkan 2 ensemble (low-vol & high-vol) dengan gating volatilitas.\"\"\"\n",
        "\n",
        "    def __init__(self, include_xgb: bool = True):\n",
        "        self.include_xgb = include_xgb\n",
        "        self.low_model = build_adaptive_quant_ensemble(include_xgb=include_xgb)\n",
        "        self.high_model = build_adaptive_quant_ensemble(include_xgb=include_xgb)\n",
        "        self.regime_center_ = 0.0\n",
        "        self.regime_scale_ = 1.0\n",
        "        self.regime_col_ = \"Volatility_20\"\n",
        "\n",
        "    def _extract_regime_series(self, X):\n",
        "        if hasattr(X, \"columns\") and self.regime_col_ in X.columns:\n",
        "            ser = X[self.regime_col_].to_numpy(dtype=float)\n",
        "        else:\n",
        "            # fallback jika kolom tidak ada / input ndarray\n",
        "            arr = np.asarray(X, dtype=float)\n",
        "            ser = arr[:, -1] if arr.ndim == 2 and arr.shape[1] > 0 else np.zeros(len(arr), dtype=float)\n",
        "        ser = np.nan_to_num(ser, nan=np.nanmedian(ser) if len(ser) else 0.0, posinf=0.0, neginf=0.0)\n",
        "        return ser\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_arr = np.asarray(y)\n",
        "        regime = self._extract_regime_series(X)\n",
        "        self.regime_center_ = float(np.nanmedian(regime))\n",
        "        self.regime_scale_ = float(np.nanstd(regime)) if np.nanstd(regime) > 1e-9 else 1.0\n",
        "\n",
        "        high_mask = regime >= self.regime_center_\n",
        "        low_mask = ~high_mask\n",
        "\n",
        "        # fallback jika salah satu regime terlalu sedikit\n",
        "        if np.sum(low_mask) < 20 or np.sum(high_mask) < 20:\n",
        "            self.low_model.fit(X, y_arr)\n",
        "            self.high_model.fit(X, y_arr)\n",
        "            return self\n",
        "\n",
        "        if hasattr(X, \"iloc\"):\n",
        "            self.low_model.fit(X.iloc[low_mask], y_arr[low_mask])\n",
        "            self.high_model.fit(X.iloc[high_mask], y_arr[high_mask])\n",
        "        else:\n",
        "            X_arr = np.asarray(X)\n",
        "            self.low_model.fit(X_arr[low_mask], y_arr[low_mask])\n",
        "            self.high_model.fit(X_arr[high_mask], y_arr[high_mask])\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        p_low = self.low_model.predict_proba(X)\n",
        "        p_high = self.high_model.predict_proba(X)\n",
        "\n",
        "        regime = self._extract_regime_series(X)\n",
        "        z = (regime - self.regime_center_) / self.regime_scale_\n",
        "        gate_high = 1 / (1 + np.exp(-z))\n",
        "        gate_high = np.clip(gate_high, 0.05, 0.95)\n",
        "\n",
        "        p_up = (1 - gate_high) * p_low[:, 1] + gate_high * p_high[:, 1]\n",
        "        p_up = np.clip(p_up, 0.01, 0.99)\n",
        "        return np.column_stack([1 - p_up, p_up])\n",
        "\n",
        "\n",
        "def get_model_candidates():\n",
        "    candidates = {\n",
        "        \"RegimeAwareAdaptiveQuant\": RegimeAwareAdaptiveQuant(include_xgb=True),\n",
        "        \"AdaptiveQuantEnsemble\": build_adaptive_quant_ensemble(include_xgb=True),\n",
        "        \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
        "            learning_rate=0.03,\n",
        "            max_depth=4,\n",
        "            max_iter=400,\n",
        "            min_samples_leaf=20,\n",
        "            random_state=SEED,\n",
        "        ),\n",
        "        \"ExtraTrees\": ExtraTreesClassifier(\n",
        "            n_estimators=700,\n",
        "            max_depth=10,\n",
        "            min_samples_leaf=6,\n",
        "            class_weight=\"balanced_subsample\",\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "        ),\n",
        "        \"LogisticRegression\": Pipeline(\n",
        "            steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=SEED)),\n",
        "            ]\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    if TARGET_MODE != \"three_class\":\n",
        "        try:\n",
        "            from xgboost import XGBClassifier\n",
        "\n",
        "            candidates[\"XGBoost\"] = XGBClassifier(\n",
        "                n_estimators=500,\n",
        "                learning_rate=0.03,\n",
        "                max_depth=4,\n",
        "                subsample=0.9,\n",
        "                colsample_bytree=0.9,\n",
        "                reg_lambda=1.0,\n",
        "                objective=\"binary:logistic\",\n",
        "                eval_metric=\"logloss\",\n",
        "                random_state=SEED,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def map_target_label_to_text(y: np.ndarray) -> np.ndarray:\n",
        "    y_arr = np.asarray(y)\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        mapping = {0: \"TURUN\", 1: \"SIDEWAYS\", 2: \"NAIK\"}\n",
        "        return np.array([mapping.get(int(v), str(v)) for v in y_arr])\n",
        "    return np.where(y_arr == 1, \"NAIK\", \"TURUN\")\n",
        "\n",
        "\n",
        "def extract_direction_probs(model, proba: np.ndarray):\n",
        "    classes = list(getattr(model, \"classes_\", []))\n",
        "    if TARGET_MODE == \"three_class\" and len(classes) >= 3:\n",
        "        p_down = proba[:, classes.index(0)] if 0 in classes else np.zeros(len(proba))\n",
        "        p_side = proba[:, classes.index(1)] if 1 in classes else np.zeros(len(proba))\n",
        "        p_up = proba[:, classes.index(2)] if 2 in classes else np.zeros(len(proba))\n",
        "        return p_down, p_side, p_up\n",
        "\n",
        "    # binary fallback\n",
        "    p_up = proba[:, 1] if proba.shape[1] > 1 else proba[:, 0]\n",
        "    p_down = 1 - p_up\n",
        "    p_side = np.zeros(len(p_up))\n",
        "    return p_down, p_side, p_up\n",
        "\n",
        "\n",
        "def decide_signal_three_class(prob_down: float, prob_side: float, prob_up: float, threshold: float) -> str:\n",
        "    if prob_up >= max(threshold, prob_down, prob_side):\n",
        "        return \"BELI\"\n",
        "    if prob_down >= max(threshold, prob_up, prob_side):\n",
        "        return \"JUAL\"\n",
        "    return \"TAHAN\"\n",
        "\n",
        "\n",
        "def evaluate_model(name, model, X_train, y_train, X_val, y_val):\n",
        "    fitted = model\n",
        "    fitted.fit(X_train, y_train)\n",
        "\n",
        "    if USE_PROB_CALIBRATION:\n",
        "        try:\n",
        "            calibrated = CalibratedClassifierCV(estimator=fitted, method=\"sigmoid\", cv=3)\n",
        "            calibrated.fit(X_train, y_train)\n",
        "            fitted = calibrated\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    proba = np.nan_to_num(fitted.predict_proba(X_val), nan=0.5, posinf=1.0, neginf=0.0)\n",
        "    p_down, p_side, p_up = extract_direction_probs(fitted, proba)\n",
        "\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        val_preds = np.asarray(getattr(fitted, \"classes_\", np.array([0, 1, 2])))[np.argmax(proba, axis=1)]\n",
        "        threshold = 0.5\n",
        "        val_auc = float(\"nan\")\n",
        "    else:\n",
        "        threshold = find_best_threshold(y_val, p_up)\n",
        "        val_preds = (p_up >= threshold).astype(int)\n",
        "        val_auc = safe_auc(y_val, p_up)\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"model\": fitted,\n",
        "        \"threshold\": threshold,\n",
        "        \"val_auc\": val_auc,\n",
        "        \"val_bacc\": balanced_accuracy_score(y_val, val_preds),\n",
        "        \"val_acc\": accuracy_score(y_val, val_preds),\n",
        "    }\n",
        "\n",
        "\n",
        "def estimate_expected_return(prob_up: float, return_series: pd.Series) -> float:\n",
        "    up_returns = return_series[return_series > 0]\n",
        "    down_returns = return_series[return_series <= 0]\n",
        "    mean_up = float(up_returns.mean()) if len(up_returns) else 0.0\n",
        "    mean_down = float(down_returns.mean()) if len(down_returns) else 0.0\n",
        "    return (prob_up * mean_up) + ((1 - prob_up) * mean_down)\n",
        "\n",
        "\n",
        "def suggest_stoploss(signal: str, last_close: float, atr_value: float, prob_up: float, base_mult: float = 1.5):\n",
        "    if signal == \"TAHAN\":\n",
        "        return None, None, \"Tidak ada stop-loss karena sinyal TAHAN\"\n",
        "\n",
        "    confidence = abs(prob_up - 0.5) * 2\n",
        "    multiplier = base_mult + (0.7 * confidence)\n",
        "\n",
        "    if atr_value is None or np.isnan(atr_value) or atr_value <= 0:\n",
        "        fallback_pct = 0.03\n",
        "        if signal == \"BELI\":\n",
        "            stop = last_close * (1 - fallback_pct)\n",
        "            return stop, fallback_pct * 100, \"Fallback 3% (ATR tidak valid)\"\n",
        "        stop = last_close * (1 + fallback_pct)\n",
        "        return stop, fallback_pct * 100, \"Fallback 3% (ATR tidak valid, skenario short)\"\n",
        "\n",
        "    if signal == \"BELI\":\n",
        "        stop = last_close - (multiplier * atr_value)\n",
        "        stop_pct = ((last_close - stop) / last_close) * 100\n",
        "        return stop, stop_pct, f\"ATR x {multiplier:.2f} di bawah harga masuk\"\n",
        "\n",
        "    stop = last_close + (multiplier * atr_value)\n",
        "    stop_pct = ((stop - last_close) / last_close) * 100\n",
        "    return stop, stop_pct, f\"ATR x {multiplier:.2f} di atas harga referensi\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_next_bej_session_timestamps(start_ts: pd.Timestamp, periods: int) -> pd.DatetimeIndex:\n",
        "    ts = pd.Timestamp(start_ts)\n",
        "    if ts.tzinfo is None:\n",
        "        ts = ts.tz_localize(JKT_TZ)\n",
        "    else:\n",
        "        ts = ts.tz_convert(JKT_TZ)\n",
        "\n",
        "    session_hours = [9, 10, 11, 12, 14, 15, 16]\n",
        "    out = []\n",
        "    cur = ts\n",
        "\n",
        "    while len(out) < periods:\n",
        "        cur = cur + pd.Timedelta(hours=1)\n",
        "\n",
        "        # normalisasi menit/detik ke jam bulat\n",
        "        cur = cur.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "        # jika weekend, lompat ke senin jam 09:00\n",
        "        while cur.dayofweek >= 5:\n",
        "            cur = (cur + pd.Timedelta(days=1)).replace(hour=9, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        if cur.hour in session_hours and cur.dayofweek < 5:\n",
        "            out.append(cur)\n",
        "\n",
        "    return pd.DatetimeIndex(out)\n",
        "\n",
        "\n",
        "def estimate_ihsg_influence_on_latest(model, latest_row: pd.DataFrame) -> tuple[float, float]:\n",
        "    \"\"\"Bandingkan probabilitas dengan vs tanpa fitur IHSG pada baris terakhir.\"\"\"\n",
        "    base_proba = np.nan_to_num(model.predict_proba(latest_row), nan=0.5, posinf=1.0, neginf=0.0)\n",
        "    _, _, base_up = extract_direction_probs(model, base_proba)\n",
        "    base_prob = float(base_up[0])\n",
        "\n",
        "    no_ihsg = latest_row.copy()\n",
        "    for c in [\"IHSG_Return_1\", \"IHSG_Return_5\", \"IHSG_Volatility_10\"]:\n",
        "        if c in no_ihsg.columns:\n",
        "            no_ihsg[c] = 0.0\n",
        "\n",
        "    no_ihsg_proba = np.nan_to_num(model.predict_proba(no_ihsg), nan=0.5, posinf=1.0, neginf=0.0)\n",
        "    _, _, no_ihsg_up = extract_direction_probs(model, no_ihsg_proba)\n",
        "    no_ihsg_prob = float(no_ihsg_up[0])\n",
        "    return base_prob, (base_prob - no_ihsg_prob)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def suggest_tp_sl_from_sr(signal: str, entry_price: float, support: float, resistance: float, atr_value: float | None, interval: str = \"1d\"):\n",
        "    \"\"\"\n",
        "    Menentukan take-profit dan stop-loss realistis berbasis level support/resistance.\n",
        "    - BELI: SL di bawah support (atau ATR fallback), TP mendekati resistance.\n",
        "    - JUAL: SL di atas resistance, TP mendekati support.\n",
        "    \"\"\"\n",
        "    if signal == \"TAHAN\":\n",
        "        return None, None, \"Tidak ada TP/SL karena sinyal TAHAN\"\n",
        "\n",
        "    atr_buffer = 0.5 * atr_value if atr_value is not None and not np.isnan(atr_value) and atr_value > 0 else entry_price * 0.005\n",
        "    tp_atr_mult = 1.8 if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"} else 2.5\n",
        "    sl_atr_mult = 1.2 if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"} else 1.8\n",
        "\n",
        "    if signal == \"BELI\":\n",
        "        sl_floor = entry_price - (sl_atr_mult * atr_buffer)\n",
        "        if not np.isnan(support):\n",
        "            sl = max(support - atr_buffer, sl_floor)\n",
        "        else:\n",
        "            sl = sl_floor\n",
        "\n",
        "        tp_cap = entry_price + (tp_atr_mult * atr_buffer)\n",
        "        if not np.isnan(resistance):\n",
        "            tp = min(resistance - (0.2 * atr_buffer), tp_cap)\n",
        "        else:\n",
        "            tp = tp_cap\n",
        "\n",
        "        if tp <= entry_price:\n",
        "            tp = entry_price * 1.005\n",
        "        if sl >= entry_price:\n",
        "            sl = entry_price * 0.995\n",
        "\n",
        "        return tp, sl, \"TP/SL dari resistance-support + buffer ATR (skenario long)\"\n",
        "\n",
        "    # signal == JUAL (short/defensive)\n",
        "    sl_cap = entry_price + (sl_atr_mult * atr_buffer)\n",
        "    if not np.isnan(resistance):\n",
        "        sl = min(resistance + atr_buffer, sl_cap)\n",
        "    else:\n",
        "        sl = sl_cap\n",
        "\n",
        "    tp_floor = entry_price - (tp_atr_mult * atr_buffer)\n",
        "    if not np.isnan(support):\n",
        "        tp = max(support + (0.2 * atr_buffer), tp_floor)\n",
        "    else:\n",
        "        tp = tp_floor\n",
        "\n",
        "    if tp >= entry_price:\n",
        "        tp = entry_price * 0.995\n",
        "    if sl <= entry_price:\n",
        "        sl = entry_price * 1.005\n",
        "\n",
        "    return tp, sl, \"TP/SL dari resistance-support + buffer ATR (skenario short)\"\n",
        "\n",
        "\n",
        "def suggest_entry_range(signal: str, last_close: float, support: float, resistance: float, atr_value: float | None, interval: str = \"1d\"):\n",
        "    \"\"\"Saran range entry realistis berbasis ATR + support/resistance + konteks sinyal.\"\"\"\n",
        "    atr_buffer = 0.35 * atr_value if atr_value is not None and not np.isnan(atr_value) and atr_value > 0 else last_close * 0.004\n",
        "    intraday = interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}\n",
        "    pullback_mult = 0.8 if intraday else 1.1\n",
        "    breakout_mult = 0.6 if intraday else 0.9\n",
        "\n",
        "    if signal == \"BELI\":\n",
        "        # Entry ideal saat pullback mendekati support atau area diskon dari harga terakhir.\n",
        "        base_low = last_close - (pullback_mult * atr_buffer)\n",
        "        base_high = last_close + (0.25 * atr_buffer)\n",
        "\n",
        "        if not np.isnan(support):\n",
        "            low = max(support + (0.05 * atr_buffer), base_low)\n",
        "        else:\n",
        "            low = base_low\n",
        "\n",
        "        if not np.isnan(resistance):\n",
        "            high_cap = resistance - (0.2 * atr_buffer)\n",
        "            high = min(base_high, high_cap)\n",
        "        else:\n",
        "            high = base_high\n",
        "\n",
        "        if low >= high:\n",
        "            low = last_close * 0.997\n",
        "            high = last_close * 1.003\n",
        "\n",
        "        return low, high, \"Entry BUY disarankan saat pullback (dekat support/area diskon ATR)\"\n",
        "\n",
        "    if signal == \"JUAL\":\n",
        "        # Entry ideal saat rebound mendekati resistance atau area premium dari harga terakhir.\n",
        "        base_low = last_close - (0.25 * atr_buffer)\n",
        "        base_high = last_close + (breakout_mult * atr_buffer)\n",
        "\n",
        "        if not np.isnan(support):\n",
        "            low_floor = support + (0.2 * atr_buffer)\n",
        "            low = max(base_low, low_floor)\n",
        "        else:\n",
        "            low = base_low\n",
        "\n",
        "        if not np.isnan(resistance):\n",
        "            high = min(resistance - (0.05 * atr_buffer), base_high)\n",
        "        else:\n",
        "            high = base_high\n",
        "\n",
        "        if low >= high:\n",
        "            low = last_close * 0.997\n",
        "            high = last_close * 1.003\n",
        "\n",
        "        return low, high, \"Entry SELL disarankan saat rebound (dekat resistance/area premium ATR)\"\n",
        "\n",
        "    neutral_low = last_close - (0.5 * atr_buffer)\n",
        "    neutral_high = last_close + (0.5 * atr_buffer)\n",
        "    return neutral_low, neutral_high, \"Mode TAHAN: entry agresif tidak disarankan, hanya range observasi\"\n",
        "\n",
        "\n",
        "def forecast_next_periods(last_close: float, expected_return: float, start_date: pd.Timestamp, periods: int, interval: str,\n",
        "                          return_history: pd.Series, prob_up: float) -> pd.DataFrame:\n",
        "    \"\"\"Forecast path yang lebih realistis: tidak pakai return konstan, tapi bootstrap return historis kondisional.\"\"\"\n",
        "    if interval in {\"1h\", \"60m\"}:\n",
        "        future_index = generate_next_bej_session_timestamps(start_ts=start_date, periods=periods)\n",
        "        price_col = \"Predicted_Close_1h\"\n",
        "        ret_col = \"Simulated_Return\"\n",
        "    else:\n",
        "        future_index = pd.bdate_range(start=start_date + pd.Timedelta(days=1), periods=periods)\n",
        "        price_col = \"Predicted_Close_1d\"\n",
        "        ret_col = \"Simulated_Return\"\n",
        "\n",
        "    hist = return_history.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    if len(hist) < 10:\n",
        "        simulated_returns = np.full(periods, expected_return)\n",
        "    else:\n",
        "        up_hist = hist[hist > 0]\n",
        "        down_hist = hist[hist <= 0]\n",
        "\n",
        "        if len(up_hist) == 0:\n",
        "            up_hist = hist\n",
        "        if len(down_hist) == 0:\n",
        "            down_hist = hist\n",
        "\n",
        "        rng = np.random.default_rng(SEED)\n",
        "        simulated_returns = []\n",
        "        local_prob_up = min(max(prob_up, 0.05), 0.95)\n",
        "\n",
        "        for i in range(periods):\n",
        "            pick_up = rng.random() < local_prob_up\n",
        "            if pick_up:\n",
        "                r = float(rng.choice(up_hist.values))\n",
        "            else:\n",
        "                r = float(rng.choice(down_hist.values))\n",
        "\n",
        "            # clamp agar tidak ekstrem dan tambah sedikit mean-reversion ke expected return\n",
        "            r = float(np.clip(r, -0.04, 0.04))\n",
        "            r = 0.7 * r + 0.3 * expected_return\n",
        "            simulated_returns.append(r)\n",
        "\n",
        "            # update probabilitas secara ringan supaya jalur tidak monoton\n",
        "            local_prob_up = 0.8 * local_prob_up + 0.2 * (0.5 + np.tanh(r * 20) * 0.15)\n",
        "            local_prob_up = min(max(local_prob_up, 0.2), 0.8)\n",
        "\n",
        "        simulated_returns = np.array(simulated_returns)\n",
        "\n",
        "    prices, price = [], float(last_close)\n",
        "    for r in simulated_returns:\n",
        "        price = price * (1 + r)\n",
        "        prices.append(price)\n",
        "\n",
        "    out = pd.DataFrame({\"Date\": future_index, price_col: prices, ret_col: simulated_returns})\n",
        "    out[\"Expected_Return_Base\"] = expected_return\n",
        "    return out\n",
        "\n",
        "\n",
        "def prepare_dataset(interval: str) -> tuple[pd.DataFrame, str, dict]:\n",
        "    if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"}:\n",
        "        periods_to_try = [INTRADAY_PERIOD] + [p for p in INTRADAY_FALLBACK_PERIODS if p != INTRADAY_PERIOD]\n",
        "        for period in periods_to_try:\n",
        "            raw = download_data(SYMBOL, interval, START, END, period)\n",
        "            raw = apply_as_of_cutoff(raw, AS_OF_DATE)\n",
        "            if raw.empty:\n",
        "                continue\n",
        "            base = raw[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
        "            feat = build_features(base, interval)\n",
        "            if feat.empty:\n",
        "                continue\n",
        "            min_required = MIN_ROWS_INTRADAY if interval in {\"1h\", \"60m\", \"30m\", \"15m\", \"5m\", \"1m\"} else MIN_ROWS_AFTER_FEATURES\n",
        "            if len(feat) >= min_required:\n",
        "                feat, info = add_market_context_features(feat, interval, START, END, period, AS_OF_DATE)\n",
        "                return feat, period, info\n",
        "        # fallback: pakai dataset terbaik yang masih memungkinkan split\n",
        "        best_feat, best_period = None, None\n",
        "        for period in periods_to_try:\n",
        "            raw = download_data(SYMBOL, interval, START, END, period)\n",
        "            raw = apply_as_of_cutoff(raw, AS_OF_DATE)\n",
        "            if raw.empty:\n",
        "                continue\n",
        "            base = raw[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
        "            feat = build_features(base, interval)\n",
        "            if feat.empty:\n",
        "                continue\n",
        "            if best_feat is None or len(feat) > len(best_feat):\n",
        "                best_feat, best_period = feat, period\n",
        "        if best_feat is not None and len(best_feat) >= 12:\n",
        "            best_feat, info = add_market_context_features(best_feat, interval, START, END, best_period, AS_OF_DATE)\n",
        "            return best_feat, f\"{best_period} (best-effort)\", info\n",
        "        raise ValueError(\n",
        "            f\"Data intraday terlalu sedikit setelah feature engineering. Coba period lebih besar. Tried={periods_to_try}\"\n",
        "        )\n",
        "\n",
        "    raw = download_data(SYMBOL, interval, START, END, INTRADAY_PERIOD)\n",
        "    raw = apply_as_of_cutoff(raw, AS_OF_DATE)\n",
        "    if raw.empty:\n",
        "        raise ValueError(\n",
        "            \"Data harian kosong dari Yahoo Finance. Cek SYMBOL/INTERVAL, atau perlebar START/END.\"\n",
        "        )\n",
        "\n",
        "    base = raw[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
        "    feat = build_features(base, interval)\n",
        "    if feat.empty or len(feat) < MIN_ROWS_AFTER_FEATURES:\n",
        "        raise ValueError(\n",
        "            f\"Data harian terlalu sedikit setelah feature engineering: {len(feat)} baris. \"\n",
        "            \"Coba perlebar START/END atau ganti saham dengan histori lebih lengkap.\"\n",
        "        )\n",
        "\n",
        "    feat, info = add_market_context_features(feat, interval, START, END, INTRADAY_PERIOD, AS_OF_DATE)\n",
        "    if feat.empty:\n",
        "        raise ValueError(\n",
        "            \"Semua baris hilang setelah penggabungan fitur konteks market. \"\n",
        "            \"Coba nonaktifkan sumber eksternal yang bermasalah atau ganti simbol.\"\n",
        "        )\n",
        "    return feat, \"start/end\", info\n",
        "\n",
        "\n",
        "def main():\n",
        "    mode_config = get_runtime_mode_config()\n",
        "    interval = mode_config[\"interval\"]\n",
        "\n",
        "    df, data_window_used, market_info = prepare_dataset(interval)\n",
        "    train_df, val_df, test_df = split_time_series(df)\n",
        "    feature_cols = [c for c in df.columns if c != \"Target\"]\n",
        "\n",
        "    X_train, y_train = train_df[feature_cols], train_df[\"Target\"].to_numpy()\n",
        "    X_val, y_val = val_df[feature_cols], val_df[\"Target\"].to_numpy()\n",
        "    X_test, y_test = test_df[feature_cols], test_df[\"Target\"].to_numpy()\n",
        "\n",
        "    X_train = replace_inf_with_nan(X_train)\n",
        "    X_val = replace_inf_with_nan(X_val)\n",
        "    X_test = replace_inf_with_nan(X_test)\n",
        "\n",
        "    candidates = get_model_candidates()\n",
        "    evaluations = [evaluate_model(name, model, X_train, y_train, X_val, y_val) for name, model in candidates.items()]\n",
        "    best = max(evaluations, key=lambda x: (x[\"val_bacc\"], np.nan_to_num(x[\"val_auc\"], nan=-1.0)))\n",
        "\n",
        "    best_model = best[\"model\"]\n",
        "    best_threshold = best[\"threshold\"]\n",
        "\n",
        "    test_proba = np.nan_to_num(best_model.predict_proba(X_test), nan=0.5, posinf=1.0, neginf=0.0)\n",
        "    probs_down, probs_side, probs_up = extract_direction_probs(best_model, test_proba)\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        preds = np.asarray(getattr(best_model, \"classes_\", np.array([0, 1, 2])))[np.argmax(test_proba, axis=1)]\n",
        "    else:\n",
        "        preds = (probs_up >= best_threshold).astype(int)\n",
        "\n",
        "    print(f\"Mode trading : {mode_config['name']} ({mode_config['horizon_note']})\")\n",
        "    print(f\"Mode interval: {interval}\")\n",
        "    print(f\"Data window used: {data_window_used}\")\n",
        "    print(f\"As-of cutoff: {AS_OF_DATE if AS_OF_DATE else 'latest available'}\")\n",
        "    print(f\"IHSG context used: {market_info.get('ihsg_used')}\")\n",
        "    print(f\"Global markets used: {market_info.get('global_markets_used', 0)}/{len(GLOBAL_INDEX_TICKERS)}\")\n",
        "    print(f\"Commodities used  : {market_info.get('commodities_used', 0)}/{len(COMMODITY_TICKERS)}\")\n",
        "    print(f\"INDO VIX source : {market_info.get('indovix_symbol') if market_info.get('indovix_symbol') else 'not found (filled neutral)'}\")\n",
        "    print(f\"USD/IDR context: {market_info.get('currency_used')}\")\n",
        "    print(f\"Timestamp aligned: {market_info.get('timestamps_aligned')}\")\n",
        "    print(f\"VIX fear rules  : spike>={VIX_SPIKE_THRESHOLD:.2%}, high_level>={VIX_HIGH_LEVEL}\")\n",
        "    print(f\"BEI sessions    : {SESSION1_START}-{SESSION1_END} & {SESSION2_START}-{SESSION2_END} ({JKT_TZ})\")\n",
        "    print(\"Model candidates (validation):\")\n",
        "    for ev in evaluations:\n",
        "        auc_text = \"nan\" if np.isnan(ev[\"val_auc\"]) else f\"{ev['val_auc']:.4f}\"\n",
        "        print(\n",
        "            f\"- {ev['name']}: AUC={auc_text}, \"\n",
        "            f\"BalancedAcc={ev['val_bacc']:.4f}, Acc={ev['val_acc']:.4f}, Threshold={ev['threshold']:.2f}\"\n",
        "        )\n",
        "\n",
        "    print(f\"\\nModel terpilih: {best['name']}\")\n",
        "    print(f\"Best threshold (validation): {best_threshold:.2f}\")\n",
        "    print(f\"Test accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
        "    print(f\"Test balanced accuracy: {balanced_accuracy_score(y_test, preds):.4f}\")\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        print(\"Test ROC-AUC: nan (three_class mode)\")\n",
        "    else:\n",
        "        print(f\"Test ROC-AUC: {safe_auc(y_test, probs_up):.4f}\" if len(np.unique(y_test)) > 1 else \"Test ROC-AUC: nan\")\n",
        "    print(confusion_matrix(y_test, preds))\n",
        "    print(classification_report(y_test, preds, digits=4, zero_division=0))\n",
        "\n",
        "    result = pd.DataFrame(\n",
        "        {\n",
        "            \"Date\": test_df.index,\n",
        "            \"Close\": test_df[\"Close\"].to_numpy().reshape(-1),\n",
        "            \"Prob_Naik\": probs_up,\n",
        "            \"Prob_Turun\": probs_down,\n",
        "            \"Aktual\": map_target_label_to_text(y_test),\n",
        "            \"INDOVIX_Level\": test_df[\"INDOVIX_Level\"].to_numpy().reshape(-1) if \"INDOVIX_Level\" in test_df.columns else np.nan,\n",
        "            \"INDOVIX_Change_1\": test_df[\"INDOVIX_Change_1\"].to_numpy().reshape(-1) if \"INDOVIX_Change_1\" in test_df.columns else np.nan,\n",
        "            \"Buy_Volume_Anomaly\": test_df[\"Buy_Volume_Anomaly\"].to_numpy().reshape(-1) if \"Buy_Volume_Anomaly\" in test_df.columns else np.nan,\n",
        "            \"Sell_Volume_Anomaly\": test_df[\"Sell_Volume_Anomaly\"].to_numpy().reshape(-1) if \"Sell_Volume_Anomaly\" in test_df.columns else np.nan,\n",
        "            \"Net_Volume_Anomaly\": test_df[\"Net_Volume_Anomaly\"].to_numpy().reshape(-1) if \"Net_Volume_Anomaly\" in test_df.columns else np.nan,\n",
        "            \"Volume_Anomaly_Spike\": test_df[\"Volume_Anomaly_Spike\"].to_numpy().reshape(-1) if \"Volume_Anomaly_Spike\" in test_df.columns else 0,\n",
        "            \"Support_Level\": test_df[\"Support_Level\"].to_numpy().reshape(-1) if \"Support_Level\" in test_df.columns else np.nan,\n",
        "            \"Resistance_Level\": test_df[\"Resistance_Level\"].to_numpy().reshape(-1) if \"Resistance_Level\" in test_df.columns else np.nan,\n",
        "            \"Trend_Filter\": test_df[\"Trend_Filter\"].to_numpy().reshape(-1) if \"Trend_Filter\" in test_df.columns else np.nan,\n",
        "        }\n",
        "    )\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        result[\"Prob_Sideways\"] = probs_side\n",
        "        result[\"Signal_Dasar\"] = result.apply(lambda r: decide_signal_three_class(float(r[\"Prob_Turun\"]), float(r[\"Prob_Sideways\"]), float(r[\"Prob_Naik\"]), best_threshold), axis=1)\n",
        "    else:\n",
        "        result[\"Prob_Sideways\"] = 0.0\n",
        "        result[\"Signal_Dasar\"] = result[\"Prob_Naik\"].apply(lambda p: decide_signal(float(p), best_threshold))\n",
        "\n",
        "    result[\"Trend_Status\"] = np.where(result[\"Trend_Filter\"] >= 0.5, \"UPTREND\", \"DOWNTREND\")\n",
        "    result[\"Catatan_VolumeFlow\"] = result.apply(\n",
        "        lambda r: interpret_volume_flow(\n",
        "            float(r[\"Net_Volume_Anomaly\"]) if \"Net_Volume_Anomaly\" in result.columns else np.nan,\n",
        "            float(r[\"Buy_Volume_Anomaly\"]) if \"Buy_Volume_Anomaly\" in result.columns else np.nan,\n",
        "            float(r[\"Sell_Volume_Anomaly\"]) if \"Sell_Volume_Anomaly\" in result.columns else np.nan,\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    result[[\"Signal_Akhir\", \"Catatan_VIX\"]] = result.apply(\n",
        "        lambda r: pd.Series(\n",
        "            adjust_signal_with_vix_fear(\n",
        "                signal=r[\"Signal_Dasar\"],\n",
        "                prob_up=float(r[\"Prob_Naik\"]),\n",
        "                vix_level=float(r[\"INDOVIX_Level\"]) if \"INDOVIX_Level\" in result.columns else np.nan,\n",
        "                vix_change_1=float(r[\"INDOVIX_Change_1\"]) if \"INDOVIX_Change_1\" in result.columns else np.nan,\n",
        "            )\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    result_display = result[\n",
        "        [\n",
        "            \"Date\",\n",
        "            \"Close\",\n",
        "            \"Prob_Naik\",\n",
        "            \"Prob_Turun\",\n",
        "            \"Prob_Sideways\",\n",
        "            \"Aktual\",\n",
        "            \"Trend_Status\",\n",
        "            \"INDOVIX_Level\",\n",
        "            \"Catatan_VolumeFlow\",\n",
        "            \"Signal_Akhir\",\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    print(\"\\nContoh output (10 baris terakhir):\")\n",
        "    print(result_display.tail(10).to_string(index=False))\n",
        "\n",
        "    latest_row = replace_inf_with_nan(df[feature_cols].tail(1))\n",
        "    latest_proba = np.nan_to_num(best_model.predict_proba(latest_row), nan=0.5, posinf=1.0, neginf=0.0)\n",
        "    prob_down_arr, prob_side_arr, prob_up_arr = extract_direction_probs(best_model, latest_proba)\n",
        "    prob_up_now = float(prob_up_arr[0])\n",
        "    prob_down_now = float(prob_down_arr[0])\n",
        "    prob_side_now = float(prob_side_arr[0])\n",
        "    prob_with_ihsg, delta_ihsg = estimate_ihsg_influence_on_latest(best_model, latest_row)\n",
        "    signal_now_base = decide_signal_three_class(prob_down_now, prob_side_now, prob_up_now, best_threshold) if TARGET_MODE == \"three_class\" else decide_signal(prob_up_now, best_threshold)\n",
        "    vix_level_now = float(df[\"INDOVIX_Level\"].iloc[-1]) if \"INDOVIX_Level\" in df.columns else np.nan\n",
        "    vix_change_now = float(df[\"INDOVIX_Change_1\"].iloc[-1]) if \"INDOVIX_Change_1\" in df.columns else np.nan\n",
        "    buy_vol_anom_now = float(df[\"Buy_Volume_Anomaly\"].iloc[-1]) if \"Buy_Volume_Anomaly\" in df.columns else np.nan\n",
        "    sell_vol_anom_now = float(df[\"Sell_Volume_Anomaly\"].iloc[-1]) if \"Sell_Volume_Anomaly\" in df.columns else np.nan\n",
        "    net_vol_anom_now = float(df[\"Net_Volume_Anomaly\"].iloc[-1]) if \"Net_Volume_Anomaly\" in df.columns else np.nan\n",
        "    vol_flow_note = interpret_volume_flow(net_vol_anom_now, buy_vol_anom_now, sell_vol_anom_now)\n",
        "    support_now = float(df[\"Support_Level\"].iloc[-1]) if \"Support_Level\" in df.columns else np.nan\n",
        "    resistance_now = float(df[\"Resistance_Level\"].iloc[-1]) if \"Resistance_Level\" in df.columns else np.nan\n",
        "    sr_note, sr_bias = interpret_sr_breakout(float(df[\"Close\"].iloc[-1]), support_now, resistance_now)\n",
        "    signal_now, vix_note = adjust_signal_with_vix_fear(signal_now_base, prob_up_now, vix_level_now, vix_change_now)\n",
        "    if sr_bias == -1 and signal_now == \"BELI\":\n",
        "        signal_now = \"TAHAN\"\n",
        "    if sr_bias == 1 and signal_now == \"JUAL\":\n",
        "        signal_now = \"TAHAN\"\n",
        "    atr_now = float(df[\"ATR\"].iloc[-1]) if \"ATR\" in df.columns else np.nan\n",
        "    stoploss_price, stoploss_pct, stoploss_note = suggest_stoploss(\n",
        "        signal=signal_now,\n",
        "        last_close=float(df[\"Close\"].iloc[-1]),\n",
        "        atr_value=atr_now,\n",
        "        prob_up=prob_up_now,\n",
        "    )\n",
        "    tp_price, sl_price_sr, tp_sl_note = suggest_tp_sl_from_sr(\n",
        "        signal=signal_now,\n",
        "        entry_price=float(df[\"Close\"].iloc[-1]),\n",
        "        support=support_now,\n",
        "        resistance=resistance_now,\n",
        "        atr_value=atr_now,\n",
        "        interval=interval,\n",
        "    )\n",
        "    entry_low, entry_high, entry_note = suggest_entry_range(\n",
        "        signal=signal_now,\n",
        "        last_close=float(df[\"Close\"].iloc[-1]),\n",
        "        support=support_now,\n",
        "        resistance=resistance_now,\n",
        "        atr_value=atr_now,\n",
        "        interval=interval,\n",
        "    )\n",
        "\n",
        "    print(\"\\nSignal saat ini:\")\n",
        "    print(f\"Timestamp terakhir   : {df.index[-1]}\")\n",
        "    print(f\"Prob_Naik saat ini   : {prob_up_now:.4f}\")\n",
        "    print(f\"Prob_Turun saat ini  : {prob_down_now:.4f}\")\n",
        "    if TARGET_MODE == \"three_class\":\n",
        "        print(f\"Prob_Sideways kini   : {prob_side_now:.4f}\")\n",
        "    print(f\"IHSG impact (delta)  : {delta_ihsg:+.4f} pada Prob_Naik (vs IHSG=0)\")\n",
        "    print(f\"Signal dasar         : {signal_now_base}\")\n",
        "    print(f\"INDO VIX level/change: {vix_level_now:.4f} / {vix_change_now:.4f}\")\n",
        "    print(f\"Signal saat ini      : {signal_now}\")\n",
        "    print(f\"Catatan INDO VIX     : {vix_note}\")\n",
        "    print(f\"Volume anomaly (B/S/N): {buy_vol_anom_now:.4f} / {sell_vol_anom_now:.4f} / {net_vol_anom_now:.4f}\")\n",
        "    print(f\"Catatan volume flow  : {vol_flow_note}\")\n",
        "    print(f\"Support/Resistance   : {support_now:.2f} / {resistance_now:.2f}\")\n",
        "    print(f\"Saran range entry    : {entry_low:.2f} - {entry_high:.2f}\")\n",
        "    print(f\"Catatan entry range  : {entry_note}\")\n",
        "    print(f\"Catatan breakout SR  : {sr_note}\")\n",
        "    if stoploss_price is not None:\n",
        "        print(f\"Stop-loss saran (ATR): {stoploss_price:.2f} ({stoploss_pct:.2f}%)\")\n",
        "    print(f\"Catatan stop-loss    : {stoploss_note}\")\n",
        "    if tp_price is not None and sl_price_sr is not None:\n",
        "        rr = abs((tp_price - float(df[\"Close\"].iloc[-1])) / (float(df[\"Close\"].iloc[-1]) - sl_price_sr)) if signal_now == \"BELI\" and float(df[\"Close\"].iloc[-1]) != sl_price_sr else None\n",
        "        if signal_now == \"JUAL\":\n",
        "            rr = abs((float(df[\"Close\"].iloc[-1]) - tp_price) / (sl_price_sr - float(df[\"Close\"].iloc[-1]))) if sl_price_sr != float(df[\"Close\"].iloc[-1]) else None\n",
        "        print(f\"Take-profit (SR)     : {tp_price:.2f}\")\n",
        "        print(f\"Stop-loss (SR)       : {sl_price_sr:.2f}\")\n",
        "        if rr is not None and np.isfinite(rr):\n",
        "            print(f\"Risk/Reward approx   : 1:{rr:.2f}\")\n",
        "    print(f\"Catatan TP/SL SR     : {tp_sl_note}\")\n",
        "\n",
        "    expected_ret = estimate_expected_return(prob_up_now, train_df[\"Return\"])\n",
        "\n",
        "    forecast = forecast_next_periods(\n",
        "        last_close=float(df[\"Close\"].iloc[-1]),\n",
        "        expected_return=expected_ret,\n",
        "        start_date=df.index[-1],\n",
        "        periods=mode_config[\"forecast_periods\"],\n",
        "        interval=interval,\n",
        "        return_history=train_df[\"Return\"],\n",
        "        prob_up=prob_up_now,\n",
        "    )\n",
        "    print(f\"\\n{mode_config['forecast_label']}:\")\n",
        "    print(forecast.to_string(index=False))"
      ],
      "metadata": {
        "id": "haPvqq2xJXNk"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIPGS2n4Jjyk",
        "outputId": "8c7338e5-17f9-4c2a-a14f-db0f70040736"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['^JKVIX']: YFTzMissingError('possibly delisted; no timezone found')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['JKVIX']: YFTzMissingError('possibly delisted; no timezone found')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['VIX.JK']: YFTzMissingError('possibly delisted; no timezone found')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but HistGradientBoostingClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but SimpleImputer was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but HistGradientBoostingClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but SimpleImputer was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but HistGradientBoostingClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but SimpleImputer was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode trading : mingguan (weekly swing-trading)\n",
            "Mode interval: 1d\n",
            "Data window used: start/end\n",
            "As-of cutoff: latest available\n",
            "IHSG context used: True\n",
            "Global markets used: 5/5\n",
            "Commodities used  : 2/2\n",
            "INDO VIX source : ^VIX\n",
            "USD/IDR context: True\n",
            "Timestamp aligned: True\n",
            "VIX fear rules  : spike>=4.00%, high_level>=25.0\n",
            "BEI sessions    : 09:00:00-12:00:00 & 13:30:00-16:00:00 (Asia/Jakarta)\n",
            "Model candidates (validation):\n",
            "- RegimeAwareAdaptiveQuant: AUC=nan, BalancedAcc=0.6429, Acc=0.6026, Threshold=0.50\n",
            "- AdaptiveQuantEnsemble: AUC=nan, BalancedAcc=0.6667, Acc=0.6154, Threshold=0.50\n",
            "- HistGradientBoosting: AUC=nan, BalancedAcc=1.0000, Acc=1.0000, Threshold=0.50\n",
            "- ExtraTrees: AUC=nan, BalancedAcc=0.3524, Acc=0.2308, Threshold=0.50\n",
            "- LogisticRegression: AUC=nan, BalancedAcc=0.6384, Acc=0.6026, Threshold=0.50\n",
            "\n",
            "Model terpilih: HistGradientBoosting\n",
            "Best threshold (validation): 0.50\n",
            "Test accuracy: 0.9747\n",
            "Test balanced accuracy: 0.9649\n",
            "Test ROC-AUC: nan (three_class mode)\n",
            "[[32  0  0]\n",
            " [ 2 17  0]\n",
            " [ 0  0 28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9412    1.0000    0.9697        32\n",
            "           1     1.0000    0.8947    0.9444        19\n",
            "           2     1.0000    1.0000    1.0000        28\n",
            "\n",
            "    accuracy                         0.9747        79\n",
            "   macro avg     0.9804    0.9649    0.9714        79\n",
            "weighted avg     0.9762    0.9747    0.9744        79\n",
            "\n",
            "\n",
            "Contoh output (10 baris terakhir):\n",
            "      Date  Close  Prob_Naik  Prob_Turun  Prob_Sideways   Aktual Trend_Status  INDOVIX_Level                        Catatan_VolumeFlow Signal_Akhir\n",
            "2026-02-11  234.0   0.010656    0.919260       0.070084    TURUN      UPTREND      17.650000  BUY PRESSURE DOMINAN (anomali beli kuat)         JUAL\n",
            "2026-02-12  228.0   0.906496    0.011430       0.082073     NAIK      UPTREND      20.820000               Volume flow netral/campuran        TAHAN\n",
            "2026-02-13  234.0   0.935327    0.011822       0.052851     NAIK    DOWNTREND      20.600000  BUY PRESSURE DOMINAN (anomali beli kuat)         BELI\n",
            "2026-02-18  268.0   0.010581    0.912894       0.076525    TURUN    DOWNTREND      19.620001  BUY PRESSURE DOMINAN (anomali beli kuat)         JUAL\n",
            "2026-02-19  262.0   0.010633    0.917397       0.071970    TURUN    DOWNTREND      20.230000 SELL PRESSURE DOMINAN (anomali jual kuat)         JUAL\n",
            "2026-02-20  252.0   0.888089    0.011177       0.100734     NAIK    DOWNTREND      19.090000               Volume flow netral/campuran         BELI\n",
            "2026-02-23  266.0   0.010926    0.941992       0.047082    TURUN    DOWNTREND      21.010000  BUY PRESSURE DOMINAN (anomali beli kuat)         JUAL\n",
            "2026-02-24  250.0   0.010276    0.238412       0.751312 SIDEWAYS    DOWNTREND      19.549999 SELL PRESSURE DOMINAN (anomali jual kuat)        TAHAN\n",
            "2026-02-25  248.0   0.009867    0.853326       0.136807    TURUN    DOWNTREND      17.930000               Volume flow netral/campuran         JUAL\n",
            "2026-02-26  232.0   0.199720    0.010674       0.789606 SIDEWAYS    DOWNTREND      18.629999               Volume flow netral/campuran        TAHAN\n",
            "\n",
            "Signal saat ini:\n",
            "Timestamp terakhir   : 2026-02-26 00:00:00\n",
            "Prob_Naik saat ini   : 0.1997\n",
            "Prob_Turun saat ini  : 0.0107\n",
            "Prob_Sideways kini   : 0.7896\n",
            "IHSG impact (delta)  : +0.0326 pada Prob_Naik (vs IHSG=0)\n",
            "Signal dasar         : TAHAN\n",
            "INDO VIX level/change: 18.6300 / 0.0390\n",
            "Signal saat ini      : TAHAN\n",
            "Catatan INDO VIX     : Tidak ada penyesuaian signal dari INDO VIX\n",
            "Volume anomaly (B/S/N): -0.5975 / 0.1265 / -0.7240\n",
            "Catatan volume flow  : Volume flow netral/campuran\n",
            "Support/Resistance   : 153.00 / 278.00\n",
            "Saran range entry    : 227.13 - 236.87\n",
            "Catatan entry range  : Mode TAHAN: entry agresif tidak disarankan, hanya range observasi\n",
            "Catatan breakout SR  : Masih di dalam range support-resistance\n",
            "Catatan stop-loss    : Tidak ada stop-loss karena sinyal TAHAN\n",
            "Catatan TP/SL SR     : Tidak ada TP/SL karena sinyal TAHAN\n",
            "\n",
            "Prediksi harga 1 minggu ke depan (5 hari bursa):\n",
            "      Date  Predicted_Close_1d  Simulated_Return  Expected_Return_Base\n",
            "2026-02-27          228.585086         -0.014719             -0.008129\n",
            "2026-03-02          228.027628         -0.002439             -0.008129\n",
            "2026-03-03          223.924432         -0.017994             -0.008129\n",
            "2026-03-04          217.108456         -0.030439             -0.008129\n",
            "2026-03-05          216.578987         -0.002439             -0.008129\n"
          ]
        }
      ]
    }
  ]
}